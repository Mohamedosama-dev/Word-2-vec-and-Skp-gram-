{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sc8C9W48fMCj",
    "outputId": "6c958159-f8bc-458a-aec3-8167e4ed7daf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#unzip enronsent1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kMVPuM4R2avC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import time\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(folder_path):\n",
    "    data = []\n",
    "    files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    for file in files:\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            data.append(text)\n",
    "    return data\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def preprocess(tokens):\n",
    "    stop_words = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"from\", \"of\", \"for\", \"by\", \"with\", \"about\", \"as\"]\n",
    "    punctuation = [\",\", \".\", \"!\", \"?\", \";\", \":\", \"-\", \"--\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"'\", \"\\\"\"]\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuation]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data_folder = \"D:\\\\projects\\\\jupyter\\\\gam3a\\\\sem 8\\\\NLP\\\\enronsent1\\\\enronsent1\"\n",
    "data = load_data(data_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the data\n",
    "tokenized_data = [tokenize(text) for text in data]\n",
    "preprocessed_data = [preprocess(tokens) for tokens in tokenized_data]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_data = preprocessed_data[0:10]\n",
    "val_data = preprocessed_data[11:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"glove-twitter-50\"\n",
    "model_path = f\"{model_name}.model\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    word2vec_model = api.load(model_name)\n",
    "    word2vec_model.save(model_path)\n",
    "else:\n",
    "    word2vec_model = gensim.models.Word2Vec.load(model_path)\n",
    "    \n",
    "    \n",
    "#word2vec_model = api.load(\"glove-twitter-50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_output(data, word2vec_model):\n",
    "    samples = []\n",
    "    for paragraph in data:\n",
    "        # Convert each token to word2vec embedding\n",
    "        paragraph_embeddings = [word2vec_model.get_vector(token) for token in paragraph if token in word2vec_model.key_to_index]\n",
    "        # Split each paragraph into fixed time steps\n",
    "        time_steps = 10  # set a fixed number of time steps\n",
    "        for i in range(0, len(paragraph_embeddings) - time_steps, time_steps):\n",
    "            input_sample = paragraph_embeddings[i:i+time_steps]\n",
    "            output_sample = paragraph_embeddings[i+time_steps]\n",
    "            samples.append((input_sample, output_sample))\n",
    "    # Convert samples to numpy array and reshape for input to model\n",
    "    input_data = np.array([sample[0] for sample in samples])\n",
    "    output_data = np.array([sample[1] for sample in samples])\n",
    "    return input_data, output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation input and output\n",
    "train_input, train_output = prepare_input_output(train_data, word2vec_model)\n",
    "val_input, val_output = prepare_input_output(val_data, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205733, 10, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205733, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caT4mbRwrrqm",
    "outputId": "c288db65-386c-475e-c393-f1ebad5e1992",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1608/1608 [==============================] - 9s 4ms/step - loss: 0.3045 - val_loss: 0.3001\n",
      "Epoch 2/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2980 - val_loss: 0.2962\n",
      "Epoch 3/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2958 - val_loss: 0.2942\n",
      "Epoch 4/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2943 - val_loss: 0.2942\n",
      "Epoch 5/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2932 - val_loss: 0.2939\n",
      "Epoch 6/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2923 - val_loss: 0.2931\n",
      "Epoch 7/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2914 - val_loss: 0.2934\n",
      "Epoch 8/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2905 - val_loss: 0.2929\n",
      "Epoch 9/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2896 - val_loss: 0.2934\n",
      "Epoch 10/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2886 - val_loss: 0.2938\n",
      "Epoch 11/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2876 - val_loss: 0.2937\n",
      "Epoch 12/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2865 - val_loss: 0.2948\n",
      "Epoch 13/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2854 - val_loss: 0.2950\n",
      "Epoch 14/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2841 - val_loss: 0.2964\n",
      "Epoch 15/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2829 - val_loss: 0.2973\n",
      "Epoch 16/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2815 - val_loss: 0.2985\n",
      "Epoch 17/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2800 - val_loss: 0.2993\n",
      "Epoch 18/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2786 - val_loss: 0.3009\n",
      "Epoch 19/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2771 - val_loss: 0.3030\n",
      "Epoch 20/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2756 - val_loss: 0.3028\n",
      "Epoch 21/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2740 - val_loss: 0.3060\n",
      "Epoch 22/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2725 - val_loss: 0.3066\n",
      "Epoch 23/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2711 - val_loss: 0.3092\n",
      "Epoch 24/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2697 - val_loss: 0.3104\n",
      "Epoch 25/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2684 - val_loss: 0.3112\n",
      "Epoch 26/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2671 - val_loss: 0.3120\n",
      "Epoch 27/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2658 - val_loss: 0.3141\n",
      "Epoch 28/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2646 - val_loss: 0.3169\n",
      "Epoch 29/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2635 - val_loss: 0.3164\n",
      "Epoch 30/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2623 - val_loss: 0.3186\n",
      "Epoch 31/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2613 - val_loss: 0.3193\n",
      "Epoch 32/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2603 - val_loss: 0.3210\n",
      "Epoch 33/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2594 - val_loss: 0.3218\n",
      "Epoch 34/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2584 - val_loss: 0.3227\n",
      "Epoch 35/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2575 - val_loss: 0.3237\n",
      "Epoch 36/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2567 - val_loss: 0.3265\n",
      "Epoch 37/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2558 - val_loss: 0.3276\n",
      "Epoch 38/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2551 - val_loss: 0.3263\n",
      "Epoch 39/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2543 - val_loss: 0.3282\n",
      "Epoch 40/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2536 - val_loss: 0.3294\n",
      "Epoch 41/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2529 - val_loss: 0.3290\n",
      "Epoch 42/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2523 - val_loss: 0.3307\n",
      "Epoch 43/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2517 - val_loss: 0.3319\n",
      "Epoch 44/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2510 - val_loss: 0.3336\n",
      "Epoch 45/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2505 - val_loss: 0.3338\n",
      "Epoch 46/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2500 - val_loss: 0.3341\n",
      "Epoch 47/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2494 - val_loss: 0.3359\n",
      "Epoch 48/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2489 - val_loss: 0.3352\n",
      "Epoch 49/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2485 - val_loss: 0.3351\n",
      "Epoch 50/50\n",
      "1608/1608 [==============================] - 6s 4ms/step - loss: 0.2480 - val_loss: 0.3383\n",
      "Training time: 5.099514321486155 minutes and 5.9708592891693115 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(train_input.shape[1], train_input.shape[2])))\n",
    "    model.add(Dense(word2vec_model.vector_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "t = time.time()\n",
    "with tf.device('/GPU:0'):\n",
    "    # Train the model\n",
    "    model.fit(train_input, train_output, validation_data=(val_input, val_output), epochs=50, batch_size=128)\n",
    "\n",
    "time_tak = time.time()-t\n",
    "print('Training time: {} minutes and {} seconds'.format(time_tak/60, time_tak%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKbEz_2kkzvZ",
    "outputId": "0b933566-41b0-4312-9c35-440766fb5575",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first word (-1 to terminate)you\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'have'?\n",
      "Yes or No? yes\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'any'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: have\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'any'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: been\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'availability'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: requested\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'it'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: to\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'any'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: provide\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'pricing'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: feedback\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'you'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: -1\n",
      "Your final sentence is:  you have have been requested to provide feedback\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sentence = []\n",
    "    word = input(\"Enter the first word: \")\n",
    "    if word == '-1':\n",
    "        break\n",
    "    sentence.append(word)\n",
    "    while True:\n",
    "        # Check if the input word is in the vocabulary\n",
    "        if word not in word2vec_model.key_to_index:\n",
    "            word = input(\"Word not found in vocabulary, please enter another word: \")\n",
    "            continue\n",
    "        input_sample = [word2vec_model.get_vector(word)]\n",
    "        # Repeat the input sample to create a batch of sequences with length 10\n",
    "        input_sample = np.array([input_sample]*10)\n",
    "        # Swap the first two dimensions of the input sample\n",
    "        input_sample = np.swapaxes(input_sample, 0, 1)\n",
    "        predicted_output = model.predict(input_sample)\n",
    "        next_word_vec = predicted_output[0]\n",
    "        next_word = word2vec_model.most_similar(positive=[next_word_vec], topn=1)[0][0]\n",
    "        print(\"Is your next word: '{}  (-1 to terminate)'?\".format(next_word))\n",
    "        correct = input(\"Yes or No? \")\n",
    "        if correct.lower() == 'yes':\n",
    "            sentence.append(next_word)\n",
    "            word = next_word\n",
    "        else:\n",
    "            word = input(\"Please enter the correct word: \")\n",
    "            sentence.append(word)\n",
    "        if word == '-1':\n",
    "            break\n",
    "            \n",
    "    if '-1' in sentence:\n",
    "        sentence.remove('-1')\n",
    "    print(\"Your final sentence is: \", ' '.join(sentence))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyYWKe38lrXZ"
   },
   "source": [
    "#so the three sentences ghich we the mode made:<br>\n",
    "\n",
    "1- You have been requested to provide feedback <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2mFKl3wkzyx"
   },
   "source": [
    "# USE skp gram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new model name and model path\n",
    "model_name = \"skp_gram\"\n",
    "model_path = f\"{model_name}.model\"\n",
    "\n",
    "# Train the word embeddings using Skip-gram if the model doesn't exist\n",
    "if not os.path.exists(model_path):\n",
    "\n",
    "    # Set the desired dimensionality of the embeddings and the context window size\n",
    "    embedding_size = 100\n",
    "    #window_size = 5\n",
    "\n",
    "    # Initialize and train the Skip-gram model\n",
    "    word2vec_model2 = Word2Vec(tokenized_data, window=window_size, sg=1)\n",
    "    word2vec_model2.save(model_path)\n",
    "else:\n",
    "    # Load the pre-trained Skip-gram model\n",
    "    word2vec_model2 = gensim.models.Word2Vec.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_output2(data, word2vec_model):\n",
    "    samples = []\n",
    "    for paragraph in data:\n",
    "        # Convert each token to word2vec embedding\n",
    "        paragraph_embeddings = [word2vec_model.wv.get_vector(token) for token in paragraph if token in word2vec_model.wv.key_to_index]\n",
    "        # Split each paragraph into fixed time steps\n",
    "        time_steps = 10  # Set a fixed number of time steps\n",
    "        for i in range(0, len(paragraph_embeddings) - time_steps, time_steps):\n",
    "            input_sample = paragraph_embeddings[i:i + time_steps]\n",
    "            output_sample = paragraph_embeddings[i + time_steps]\n",
    "            samples.append((input_sample, output_sample))\n",
    "    # Convert samples to numpy array and reshape for input to model\n",
    "    input_data = np.array([sample[0] for sample in samples])\n",
    "    output_data = np.array([sample[1] for sample in samples])\n",
    "    return input_data, output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation input and output\n",
    "train_input2, train_output2 = prepare_input_output2(train_data, word2vec_model2)\n",
    "val_input2, val_output2 = prepare_input_output2(val_data, word2vec_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1864/1864 [==============================] - 8s 4ms/step - loss: 0.0535 - val_loss: 0.0559\n",
      "Epoch 2/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0519 - val_loss: 0.0553\n",
      "Epoch 3/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0515 - val_loss: 0.0550\n",
      "Epoch 4/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0512 - val_loss: 0.0546\n",
      "Epoch 5/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0509 - val_loss: 0.0544\n",
      "Epoch 6/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0507 - val_loss: 0.0541\n",
      "Epoch 7/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0505 - val_loss: 0.0544\n",
      "Epoch 8/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0503 - val_loss: 0.0541\n",
      "Epoch 9/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0501 - val_loss: 0.0541\n",
      "Epoch 10/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0500 - val_loss: 0.0540\n",
      "Epoch 11/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0499 - val_loss: 0.0539\n",
      "Epoch 12/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0497 - val_loss: 0.0538\n",
      "Epoch 13/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0496 - val_loss: 0.0537\n",
      "Epoch 14/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0495 - val_loss: 0.0537\n",
      "Epoch 15/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0494 - val_loss: 0.0537\n",
      "Epoch 16/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0493 - val_loss: 0.0540\n",
      "Epoch 17/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0492 - val_loss: 0.0538\n",
      "Epoch 18/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0491 - val_loss: 0.0539\n",
      "Epoch 19/50\n",
      "1864/1864 [==============================] - 6s 3ms/step - loss: 0.0490 - val_loss: 0.0541\n",
      "Epoch 20/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0488 - val_loss: 0.0540\n",
      "Epoch 21/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0487 - val_loss: 0.0542\n",
      "Epoch 22/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0486 - val_loss: 0.0544\n",
      "Epoch 23/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0484 - val_loss: 0.0544\n",
      "Epoch 24/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0483 - val_loss: 0.0549\n",
      "Epoch 25/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0482 - val_loss: 0.0547\n",
      "Epoch 26/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0480 - val_loss: 0.0546\n",
      "Epoch 27/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0479 - val_loss: 0.0551\n",
      "Epoch 28/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0477 - val_loss: 0.0551\n",
      "Epoch 29/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0476 - val_loss: 0.0554\n",
      "Epoch 30/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0474 - val_loss: 0.0555\n",
      "Epoch 31/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0473 - val_loss: 0.0559\n",
      "Epoch 32/50\n",
      "1864/1864 [==============================] - 8s 4ms/step - loss: 0.0471 - val_loss: 0.0556\n",
      "Epoch 33/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0469 - val_loss: 0.0559\n",
      "Epoch 34/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0468 - val_loss: 0.0564\n",
      "Epoch 35/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0466 - val_loss: 0.0564\n",
      "Epoch 36/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0465 - val_loss: 0.0565\n",
      "Epoch 37/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0463 - val_loss: 0.0571\n",
      "Epoch 38/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0461 - val_loss: 0.0569\n",
      "Epoch 39/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0460 - val_loss: 0.0571\n",
      "Epoch 40/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0458 - val_loss: 0.0575\n",
      "Epoch 41/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0457 - val_loss: 0.0577\n",
      "Epoch 42/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0455 - val_loss: 0.0575\n",
      "Epoch 43/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0454 - val_loss: 0.0577\n",
      "Epoch 44/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0452 - val_loss: 0.0581\n",
      "Epoch 45/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0451 - val_loss: 0.0579\n",
      "Epoch 46/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0450 - val_loss: 0.0578\n",
      "Epoch 47/50\n",
      "1864/1864 [==============================] - 8s 4ms/step - loss: 0.0448 - val_loss: 0.0587\n",
      "Epoch 48/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0447 - val_loss: 0.0592\n",
      "Epoch 49/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0445 - val_loss: 0.0588\n",
      "Epoch 50/50\n",
      "1864/1864 [==============================] - 7s 4ms/step - loss: 0.0444 - val_loss: 0.0588\n",
      "Training time: 6.019236179192861 minutes and 1.1541707515716553 seconds\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(train_input2.shape[1], train_input2.shape[2])))\n",
    "    model.add(Dense(word2vec_model2.vector_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "t = time.time()\n",
    "with tf.device('/GPU:0'):\n",
    "    # Train the model\n",
    "    model.fit(train_input2, train_output2, validation_data=(val_input2, val_output2), epochs=50, batch_size=128)\n",
    "\n",
    "time_tak = time.time()-t\n",
    "print('Training time: {} minutes and {} seconds'.format(time_tak/60, time_tak%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the first word: you\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "Is your next word: 'come.  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: have\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Is your next word: 'e  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: been\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'creating  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: requested\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Is your next word: 'e  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: to\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'present  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: provide\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Is your next word: 'monthly  (-1 to terminate)'?\n",
      "Yes or No? no\n",
      "Please enter the correct word: feedback\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Is your next word: 'process.  (-1 to terminate)'?\n",
      "Yes or No? -1\n",
      "Please enter the correct word: -1\n",
      "Your final sentence is:  you have been requested to provide feedback\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sentence = []\n",
    "    word = input(\"Enter the first word: \")\n",
    "    if word == '-1':\n",
    "        break\n",
    "    sentence.append(word)\n",
    "    while True:\n",
    "        # Check if the input word is in the vocabulary\n",
    "        if word not in word2vec_model2.wv.key_to_index:\n",
    "            word = input(\"Word not found in vocabulary, please enter another word: \")\n",
    "            continue\n",
    "        input_sample = [word2vec_model2.wv.get_vector(word)]\n",
    "        # Repeat the input sample to create a batch of sequences with length 10\n",
    "        input_sample = np.array([input_sample] * 10)\n",
    "        # Swap the first two dimensions of the input sample\n",
    "        input_sample = np.swapaxes(input_sample, 0, 1)\n",
    "        predicted_output = model.predict(input_sample)\n",
    "        next_word_vec = predicted_output[0]\n",
    "        next_word = word2vec_model2.wv.most_similar(positive=[next_word_vec], topn=1)[0][0]\n",
    "        print(\"Is your next word: '{}  (-1 to terminate)'?\".format(next_word))\n",
    "        correct = input(\"Yes or No? \")\n",
    "        if correct.lower() == 'yes':\n",
    "            sentence.append(next_word)\n",
    "            word = next_word\n",
    "        else:\n",
    "            word = input(\"Please enter the correct word: \")\n",
    "            sentence.append(word)\n",
    "        if word == '-1':\n",
    "            break\n",
    "\n",
    "    if '-1' in sentence:\n",
    "        sentence.remove('-1')\n",
    "    print(\"Your final sentence is: \", ' '.join(sentence))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
